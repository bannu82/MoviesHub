{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2902e436-8734-4e3c-873a-306fa2bd80c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87a7dd0d-83e4-416b-add2-8829b3a3c159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pass 1  : ________________________________\n",
      "Pass 2  : ________________________________\n",
      "Pass 3  : ________________________________\n",
      "Pass 4  : ________________________________\n",
      "Pass 5  : ________________________________\n",
      "Pass 6  : ________________________________\n",
      "Pass 7  : ________________________________\n",
      "Pass 8  : ________________________________\n",
      "Pass 9  : ________________________________\n",
      "Pass 10  : ________________________________\n",
      "Pass 11  : ________________________________\n",
      "Pass 12  : ________________________________\n",
      "Pass 13  : ________________________________\n",
      "Pass 14  : ________________________________\n",
      "Pass 15  : ________________________________\n",
      "Pass 16  : ________________________________\n",
      "Pass 17  : ________________________________\n",
      "Pass 18  : ________________________________\n",
      "Pass 19  : ________________________________\n",
      "Pass 20  : ________________________________\n",
      "Pass 21  : ________________________________\n",
      "Pass 22  : ________________________________\n",
      "Pass 23  : ________________________________\n",
      "Pass 24  : ________________________________\n",
      "Pass 25  : ________________________________\n",
      "Pass 26  : ________________________________\n",
      "Pass 27  : ________________________________\n",
      "Pass 28  : ________________________________\n",
      "Pass 29  : ________________________________\n",
      "Pass 30  : ________________________________\n",
      "Pass 31  : ________________________________\n",
      "Pass 32  : ________________________________\n",
      "Pass 33  : ________________________________\n",
      "Pass 34  : ________________________________\n",
      "Pass 35  : ________________________________\n",
      "Pass 36  : ________________________________\n",
      "Pass 37  : ________________________________\n",
      "Pass 38  : ________________________________\n",
      "Pass 39  : ________________________________\n",
      "Pass 40  : ________________________________\n",
      "Pass 41  : ________________________________\n",
      "Pass 42  : ________________________________\n",
      "Pass 43  : ________________________________\n",
      "Pass 44  : ________________________________\n",
      "Pass 45  : ________________________________\n",
      "Pass 46  : ________________________________\n",
      "Pass 47  : ________________________________\n",
      "Pass 48  : ________________________________\n",
      "Pass 49  : ________________________________\n",
      "Pass 50  : ________________________________"
     ]
    }
   ],
   "source": [
    "Id = []\n",
    "Title=[]\n",
    "Time = []\n",
    "Type = []\n",
    "Duration =[]\n",
    "ImgLink = []\n",
    "\n",
    "movieDesc =[]\n",
    "Released =[]\n",
    "Genre = []\n",
    "Casts = []\n",
    "Country = []\n",
    "Production=[]\n",
    "IMDB_rating = []\n",
    "Cover_bg = []\n",
    "\n",
    "\n",
    "type = \"movie\"\n",
    "quality = \"all\"\n",
    "year = \"all\"\n",
    "country = \"all\"\n",
    "genre =\"all\"\n",
    "\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36'}\n",
    "\n",
    "\n",
    "for i in range(50): \n",
    "    \n",
    "    url = f\"https://moviesjoy.is/filter?type={type}&quality={quality}&release_year={year}&genre={genre}&country={country}&page={i+1}\"\n",
    "    webpage=requests.get(url, headers=headers).text\n",
    "    print(\"\\nPass\",i+1,' :' ,end=' ')\n",
    "        \n",
    "    soup= BeautifulSoup(webpage , 'lxml')\n",
    "    \n",
    "    movie = soup.find_all('div' , class_='flw-item')\n",
    "    \n",
    "    for m in movie:\n",
    "        \n",
    "        try:\n",
    "            Id.append(i+1)\n",
    "            Movie_Link =(m.find('a').get('href'))\n",
    "            Movi_Url=(f\"https://moviesjoy.is{Movie_Link}\") \n",
    "            moviePage=requests.get(Movi_Url, headers=headers).text\n",
    "            soup1 = BeautifulSoup(moviePage , 'lxml')\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        Title.append(m.find('a').get('title'))\n",
    "        Time.append(m.find(class_='fdi-item').text.strip())\n",
    "        Type.append(m.find(class_='fdi-type').text.strip())\n",
    "        Duration.append(m.find(class_='fdi-duration').text.strip())\n",
    "        ImgLink.append(m.find('div', class_='film-poster').find('img').get('data-src'))\n",
    "        \n",
    "        \n",
    "        try :\n",
    "            style =soup1.find('div',class_='cover_follow')['style']\n",
    "            Cover_bg.append(re.search('url\\((.*)\\)' , style)[1])\n",
    "            \n",
    "        except:\n",
    "            print(\"no\")\n",
    "        \n",
    "        try:\n",
    "            movieDesc.append(soup1.find('div' , class_='description').text.strip())        \n",
    "        except:\n",
    "            movieDesc.append(\"Na\")\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            IMDB_rating.append(re.split('IMDB:',soup1.find('button',class_='btn-imdb').text)[1].strip(' '))\n",
    "        except:\n",
    "            IMDB_rating.append(\"Na\")\n",
    "        \n",
    "        data_div = str(soup1.find_all('div' ,class_='row-line'))\n",
    "        \n",
    "        try:\n",
    "            Genre.append(re.findall('genre/([^\"]+)' ,data_div))        \n",
    "        except:\n",
    "            Genre.append(\"Na\")\n",
    "        \n",
    "        try:\n",
    "            Casts.append(re.findall('cast/([^\"]+)' ,data_div))        \n",
    "        except:\n",
    "            Casts.append(\"Na\")\n",
    "        \n",
    "        try:\n",
    "            Released.append(re.split('Released:\\s</strong></span>([^<]+)' ,str(data_div))[1].strip(\" \").strip('\\n'))\n",
    "        except:\n",
    "            Released.append('Na')\n",
    "        \n",
    "        try:\n",
    "            Production.append(re.findall('/production/([^\"]+)',data_div)) \n",
    "        except: \n",
    "            Production.append('Na')\n",
    "            \n",
    "        \n",
    "        try:\n",
    "            Country.append(re.split('/country/([^\"]+)',data_div)[1])\n",
    "        except:\n",
    "            Country.append(\"Na\")\n",
    "        \n",
    "        print(\"_\",end='')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f973c0c7-0c30-43d4-b2b7-593654dd5c0f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mId\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTitle\u001b[39m\u001b[38;5;124m'\u001b[39m ,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTime\u001b[39m\u001b[38;5;124m'\u001b[39m ,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mType\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDuration\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mImg\u001b[39m\u001b[38;5;124m'\u001b[39m ,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCoverBg\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmovieDesc\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReleased\u001b[39m\u001b[38;5;124m'\u001b[39m ,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGenre\u001b[39m\u001b[38;5;124m'\u001b[39m ,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCasts\u001b[39m\u001b[38;5;124m'\u001b[39m ,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCountry\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProduction\u001b[39m\u001b[38;5;124m'\u001b[39m ,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIMDB_rating\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 2\u001b[0m pair \u001b[38;5;241m=\u001b[39m [pair \u001b[38;5;28;01mfor\u001b[39;00m pair \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(Id,Title ,\u001b[43mTime\u001b[49m ,Type ,Duration, ImgLink ,Cover_bg, movieDesc ,Released ,Genre ,Casts, Country ,Production ,IMDB_rating)]\n\u001b[0;32m      3\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(pair, columns\u001b[38;5;241m=\u001b[39mcolumns )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Time' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0e8211-9d2b-46d5-a551-0f68a48ea529",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"movie.csv\",index=False, header=False , mode='a')\n",
    "# df.to_csv(\"movie.csv\",index=False, header=True )\n",
    "print(\"CSV file created successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35020961-df92-4967-950a-6cfc8b147a26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92e2bbf0-4fa6-4734-a8bb-fe1deceebddc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d925437-8c9e-4174-885d-256aa823b9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9a7d1a-a5bf-4154-a844-b2f9db1af5f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b13716f7-4151-4979-8f47-81a664334536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________\n",
      "Page 1/10 completed. Estimated time remaining: 4 minutes 24 seconds.\n",
      "________________________________\n",
      "Page 2/10 completed. Estimated time remaining: 2 minutes 58 seconds.\n",
      "________________________________\n",
      "Page 3/10 completed. Estimated time remaining: 2 minutes 19 seconds.\n",
      "________________________________\n",
      "Page 4/10 completed. Estimated time remaining: 1 minutes 51 seconds.\n",
      "________________________________\n",
      "Page 5/10 completed. Estimated time remaining: 1 minutes 28 seconds.\n",
      "________________________________\n",
      "Page 6/10 completed. Estimated time remaining: 1 minutes 8 seconds.\n",
      "________________________________\n",
      "Page 7/10 completed. Estimated time remaining: 0 minutes 50 seconds.\n",
      "________________________________\n",
      "Page 8/10 completed. Estimated time remaining: 0 minutes 33 seconds.\n",
      "________________________________\n",
      "Page 9/10 completed. Estimated time remaining: 0 minutes 16 seconds.\n",
      "________________________________\n",
      "Page 10/10 completed. Estimated time remaining: 0 minutes 0 seconds.\n",
      "\n",
      "Scraping completed in 2 minutes 43 seconds.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import requests\n",
    "import re\n",
    "import sys\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Lists to store scraped data\n",
    "Id = []\n",
    "Title = []\n",
    "Time_ = []\n",
    "Type = []\n",
    "Duration = []\n",
    "ImgLink = []\n",
    "\n",
    "movieDesc = []\n",
    "Released = []\n",
    "Genre = []\n",
    "Casts = []\n",
    "Country = []\n",
    "Production = []\n",
    "IMDB_rating = []\n",
    "Cover_bg = []\n",
    "\n",
    "# Scraping settings\n",
    "type = \"movie\"\n",
    "quality = \"all\"\n",
    "year = \"all\"\n",
    "country = \"all\"\n",
    "genre = \"all\"\n",
    "\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36'}\n",
    "\n",
    "# Start timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Number of pages to scrape\n",
    "total_pages = 5\n",
    "\n",
    "id= 0\n",
    "# Main scraping loop\n",
    "for i in range(total_pages):\n",
    "    url = f\"https://moviesjoy.is/filter?type={type}&quality={quality}&release_year={year}&genre={genre}&country={country}&page={i+1}\"\n",
    "    \n",
    "    # Fetch the webpage\n",
    "    webpage = requests.get(url, headers=headers).text\n",
    "    soup = BeautifulSoup(webpage, 'lxml')\n",
    "    \n",
    "    # Extract movies\n",
    "    movie = soup.find_all('div', class_='flw-item')\n",
    "    \n",
    "    # Time for each iteration\n",
    "    iteration_start = time.time()\n",
    "\n",
    "    for m in movie:\n",
    "        \n",
    "        try:\n",
    "\n",
    "            Movie_Link = m.find('a').get('href')\n",
    "            Movi_Url = f\"https://moviesjoy.is{Movie_Link}\"\n",
    "            moviePage = requests.get(Movi_Url, headers=headers).text\n",
    "            soup1 = BeautifulSoup(moviePage, 'lxml')\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        Id.append(id)\n",
    "        id = id+1\n",
    "        Title.append(m.find('a').get('title'))\n",
    "        Time_.append(m.find(class_='fdi-item').text.strip())\n",
    "        Type.append(m.find(class_='fdi-type').text.strip())\n",
    "        Duration.append(m.find(class_='fdi-duration').text.strip())\n",
    "        ImgLink.append(m.find('div', class_='film-poster').find('img').get('data-src'))\n",
    "        \n",
    "        try:\n",
    "            style = soup1.find('div', class_='cover_follow')['style']\n",
    "            Cover_bg.append(re.search('url\\((.*)\\)', style)[1])\n",
    "        except:\n",
    "            Cover_bg.append(\"Na\")\n",
    "\n",
    "        try:\n",
    "            movieDesc.append(soup1.find('div', class_='description').text.strip())        \n",
    "        except:\n",
    "            movieDesc.append(\"Na\")\n",
    "        \n",
    "        try:\n",
    "            IMDB_rating.append(re.split('IMDB:', soup1.find('button', class_='btn-imdb').text)[1].strip(' '))\n",
    "        except:\n",
    "            IMDB_rating.append(\"Na\")\n",
    "        \n",
    "        data_div = str(soup1.find_all('div', class_='row-line'))\n",
    "\n",
    "        try:\n",
    "            Genre.append(re.findall('genre/([^\"]+)', data_div))        \n",
    "        except:\n",
    "            Genre.append(\"Na\")\n",
    "\n",
    "        try:\n",
    "            Casts.append(re.findall('cast/([^\"]+)', data_div))        \n",
    "        except:\n",
    "            Casts.append(\"Na\")\n",
    "\n",
    "        try:\n",
    "            Released.append(re.split('Released:\\s</strong></span>([^<]+)', str(data_div))[1].strip(\" \").strip('\\n'))\n",
    "        except:\n",
    "            Released.append('Na')\n",
    "\n",
    "        try:\n",
    "            Production.append(re.findall('/production/([^\"]+)', data_div)) \n",
    "        except: \n",
    "            Production.append('Na')\n",
    "        \n",
    "        try:\n",
    "            Country.append(re.split('/country/([^\"]+)', data_div)[1])\n",
    "        except:\n",
    "            Country.append(\"Na\")\n",
    "        \n",
    "        # Progress indicator for each movie\n",
    "        sys.stdout.write('_')\n",
    "        sys.stdout.flush()\n",
    "    \n",
    "    # Calculate time taken for this iteration\n",
    "    iteration_end = time.time()\n",
    "    time_taken = iteration_end - iteration_start\n",
    "    \n",
    "    # Estimate total time remaining\n",
    "    time_elapsed = time.time() - start_time\n",
    "    estimated_total_time = (time_elapsed / (i + 1)) * total_pages\n",
    "    estimated_remaining_time = estimated_total_time - time_elapsed\n",
    "\n",
    "    # Print the progress\n",
    "    sys.stdout.write(f\"\\nPage {i + 1}/{total_pages} completed. Estimated time remaining: {int(estimated_remaining_time // 60)} minutes {int(estimated_remaining_time % 60)} seconds.\\n\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "# Total time taken\n",
    "total_time_taken = time.time() - start_time\n",
    "print(f\"\\nScraping completed in {int(total_time_taken // 60)} minutes {int(total_time_taken % 60)} seconds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8f28402-78bb-4138-a25a-099e360e71fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50c0eb49-4dbb-4b28-8b20-e5c74e3f808d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "columns = ['Id','Title' ,'Time' ,'Type', 'Duration', 'Img' ,'CoverBg','movieDesc', 'Released' ,'Genre' ,'Casts' ,'Country', 'Production' ,'IMDB_rating']\n",
    "pair = [pair for pair in zip(Id,Title ,Time_ ,Type ,Duration, ImgLink ,Cover_bg, movieDesc ,Released ,Genre ,Casts, Country ,Production ,IMDB_rating)]\n",
    "df = pd.DataFrame(pair, columns=columns )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1d1c19b-c30a-46c6-b1e1-c042b0e9418d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file created successfully!\n"
     ]
    }
   ],
   "source": [
    "# df.to_csv(\"movie.csv\",index=False, header=False , mode='a')\n",
    "df.to_csv(\"movie.csv\",index=False, header=True )\n",
    "print(\"CSV file created successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db815ca-8e94-4cb6-b762-8a00293e8e57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
